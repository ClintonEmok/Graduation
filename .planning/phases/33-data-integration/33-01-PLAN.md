---
phase: 33-data-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/db.ts
  - src/app/api/crime/stream/route.ts
  - src/app/api/crime/meta/route.ts
autonomous: true

must_haves:
  truths:
    - "DuckDB can query full CSV file directly"
    - "Date parsing handles 'MM/DD/YYYY HH:MM:SS A' format"
    - "Time range filtering works at query level"
    - "Null coordinates filtered at query level (~1.1% rows)"
  artifacts:
    - path: "src/lib/db.ts"
      provides: "DuckDB connection with CSV support"
      changes: "Add CSV file path configuration and date parsing"
    - path: "src/app/api/crime/stream/route.ts"
      provides: "Crime data stream from CSV"
      changes: "Query CSV instead of parquet, parse dates, filter nulls"
    - path: "src/app/api/crime/meta/route.ts"
      provides: "Metadata (date range, bounds, count)"
      changes: "Query CSV for real min/max dates"
  key_links:
    - from: "src/lib/db.ts"
      to: "data/*.csv"
      via: "DuckDB INSERT or direct CSV query"
      pattern: "SELECT.*FROM.*CSV"
---

<objective>
Configure DuckDB to query the full 8.5M row CSV file directly, with proper date parsing and coordinate filtering.

Purpose: Replace the 100k sample parquet with full CSV data. DuckDB can query CSV directly without preprocessing.
Output: Updated db.ts and API routes that query CSV directly.
</objective>

<execution_context>
@~/.opencode/get-shit-done/workflows/execute-plan.md
@~/.opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/33-data-integration/33-CONTEXT.md
@src/lib/db.ts
@src/app/api/crime/stream/route.ts
@src/app/api/crime/meta/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update db.ts for CSV support</name>
  <files>src/lib/db.ts</files>
  <action>
Update src/lib/db.ts to:
1. Add CSV file path constant pointing to data/Crimes_-_2001_to_Present_20260114.csv
2. Add a getDataPath() helper that returns the CSV path
3. Add helper function parseDate(dateStr: string) that handles format "01/05/2026 12:00:00 AM"
4. Export epochSeconds(dateStr: string) that parses date and returns Unix epoch seconds

The CSV uses US date format (MM/DD/YYYY) with 12-hour time and AM/PM.
Use DuckDB's parse_datetime with format '%m/%d/%Y %I:%M:%S %p'
</action>
  <verify>
Build succeeds: npm run build passes
</verify>
  <done>db.ts exports getDataPath() and epochSeconds() helpers</done>
</task>

<task type="auto">
  <name>Task 2: Update stream route to query CSV</name>
  <files>src/app/api/crime/stream/route.ts</files>
  <action>
Update /api/crime/stream to query full CSV:

1. Import getDataPath from db.ts
2. Replace parquet query with CSV query
3. Add date parsing: Convert "Date" column to timestamp using DuckDB parse_datetime
4. Add coordinate filtering: WHERE "Latitude" IS NOT NULL AND "Longitude" IS NOT NULL
5. Add column selection: Only query needed columns (Date, Primary Type, Latitude, Longitude, IUCR, District, Year)
6. Include x/z computation using lat/lon via projection: Use (lon + 87.5) / (87.7 - 87.5) for x and (lat - 37) / (42 - 37) for z (approximate Chicago bounds)
7. Handle query parameters for filtering (startDate, endDate, crimeTypes)

Query structure:
```sql
SELECT 
  epoch_seconds(parse_datetime("Date", '%m/%d/%Y %I:%M:%S %p')) as timestamp,
  "Primary Type" as type,
  "Latitude" as lat,
  "Longitude" as lon,
  ((lon + 87.5) / (87.7 - 87.5)) as x,
  ((lat - 37) / (42 - 37)) as z,
  "IUCR" as iucr,
  "District" as district,
  "Year" as year
FROM read_csv_auto('${dataPath}')
WHERE "Date" IS NOT NULL 
  AND "Latitude" IS NOT NULL 
  AND "Longitude" IS NOT NULL
```

Note: Use read_csv_auto for automatic type inference. Add filters for startDate/endDate if provided as query params.
</action>
  <verify>
Test endpoint: curl http://localhost:3000/api/crime/stream returns Arrow data with real rows
</verify>
  <done>Stream endpoint serves crime data from CSV with date parsing and coordinate filtering</done>
</task>

<task type="auto">
  <name>Task 3: Update meta route for real date range</name>
  <files>src/app/api/crime/meta/route.ts</files>
  <action>
Update /api/crime/meta to query CSV for real metadata:

1. Import getDataPath from db.ts
2. Replace parquet query with CSV query using read_csv_auto
3. Parse date column using epoch_seconds(parse_datetime("Date", '%m/%d/%Y %I:%M:%S %p'))
4. Filter nulls in query: WHERE "Date" IS NOT NULL AND "Latitude" IS NOT NULL
5. Return real minTime/maxTime as epoch seconds

Query structure:
```sql
SELECT 
  MIN(epoch_seconds(parse_datetime("Date", '%m/%d/%Y %I:%M:%S %p'))) as min_time,
  MAX(epoch_seconds(parse_datetime("Date", '%m/%d/%Y %I:%M:%S %p'))) as max_time,
  MIN("Latitude") as min_lat,
  MAX("Latitude") as max_lat,
  MIN("Longitude") as min_lon,
  MAX("Longitude") as max_lon,
  COUNT(*) as count
FROM read_csv_auto('${dataPath}')
WHERE "Date" IS NOT NULL AND "Latitude" IS NOT NULL AND "Longitude" IS NOT NULL
```

Also return distinct crime types and year range for filter options.
</action>
  <verify>
Test endpoint: curl http://localhost:3000/api/crime/meta returns real 2001-2026 date range
</verify>
  <done>Meta endpoint returns real date range (2001-2026), bounds, and count from CSV</done>
</task>

</tasks>

<verification>
- [ ] npm run build passes
- [ ] GET /api/crime/stream returns > 0 rows from CSV
- [ ] GET /api/crime/meta returns minTime ~978307200 (2001-01-01) and maxTime ~1736035200 (2025-01-05)
- [ ] No null lat/lon in results
</verification>

<success_criteria>
DuckDB queries the full 8.5M row CSV file directly. Date parsing works for "MM/DD/YYYY HH:MM:SS A" format. Stream and meta endpoints return real data.
</success_criteria>

<output>
After completion, create `.planning/phases/33-data-integration/33-01-SUMMARY.md`
</output>
