---
phase: 06
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [package.json, next.config.mjs, scripts/setup-data.js, data/README.md]
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Project has duckdb and apache-arrow dependencies installed"
    - "A script exists to generate or process the crime data"
    - "data/crime.parquet exists and contains valid data"
  artifacts:
    - path: "scripts/setup-data.js"
      provides: "Data processing logic (CSV -> Parquet)"
    - path: "data/crime.parquet"
      provides: "The source of truth for the API"
  key_links:
    - from: "scripts/setup-data.js"
      to: "duckdb"
      via: "import and execution"
---

<objective>
Setup the data engineering infrastructure by installing dependencies and creating a script to process raw crime data (or generate a large mock dataset) into an optimized Parquet file for efficient serving.

Purpose: Foundation for serving millions of points without runtime overhead.
Output: `data/crime.parquet` and configured Next.js environment.
</objective>

<execution_context>
@src/types/index.ts
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/06-data-backend-loading/06-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Dependencies</name>
  <files>package.json, next.config.mjs</files>
  <action>
    1. Install `duckdb`, `apache-arrow`, `@loaders.gl/arrow`, `@loaders.gl/core`.
    2. Configure `next.config.mjs` to include `duckdb` in `serverComponentsExternalPackages` to avoid Webpack bundling issues with the native binary.
  </action>
  <verify>
    - `npm list duckdb` returns version
    - `cat next.config.mjs` shows the update
  </verify>
  <done>Dependencies installed and config updated.</done>
</task>

<task type="auto">
  <name>Task 2: Create Data Setup Script</name>
  <files>scripts/setup-data.js</files>
  <action>
    Create a Node.js script that:
    1. Checks if `data/source.csv` exists.
    2. If NOT, generates a synthetic dataset with 100,000 rows (columns: id, lat, lon, timestamp, type) to simulate the Chicago dataset.
    3. Uses `duckdb` to convert this data into `data/crime.parquet`.
    4. Performs basic projection (Lat/Lon -> X/Z WebMercator) and normalization (Timestamp -> 0-100 Y) during the query if possible, or leaves it raw. *Decision: Keep it raw (lat/lon/time) in Parquet for flexibility, project on read or client.*
    
    Actually, for performance, let's pre-calculate X/Z/Y in the parquet to save client CPU.
    - X/Z: WebMercator projection of Lat/Lon.
    - Y: Normalized time (0-100).
    
    Ensure the script is executable via `node scripts/setup-data.js`.
  </action>
  <verify>
    - Run `node scripts/setup-data.js`
    - Check if `data/crime.parquet` is created.
  </verify>
  <done>Script exists and successfully generates parquet file.</done>
</task>

<task type="auto">
  <name>Task 3: Generate Initial Data</name>
  <files>data/crime.parquet</files>
  <action>
    Run the setup script to generate the initial parquet file.
    Create a `data/README.md` explaining the schema and how to replace `source.csv` with real data.
  </action>
  <verify>
    - `ls -lh data/crime.parquet` shows file size > 0.
  </verify>
  <done>Parquet file exists.</done>
</task>

</tasks>

<verification>
- `npm list duckdb`
- `ls data/crime.parquet`
</verification>

<success_criteria>
- Dependencies installed.
- Next.js config updated.
- `data/crime.parquet` exists and is a valid Parquet file (readable by DuckDB).
</success_criteria>

<output>
After completion, create `.planning/phases/06-data-backend-loading/06-01-SUMMARY.md`
</output>
